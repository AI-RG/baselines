# TODO (erase this)
# to do today (6/1/2018):
# make a list of all current projects and things to do for them
# (for below, further info. e.g. directories is available in yellow notepad by computer)
# Detailed (!) list of all things to finish for currently underway projects
    # finish capsule policy class
    # finish time capsule policy class
    # finish criticality-inducing regularization term (SOC)
        # See notebook
        # note: still must understand how states are handled with nsteps / nenv
    # add tensorboard logging (wouldn't that be neat!)
    # think about next steps for modularity project
        # e.g. get weighted graph from network definition
        # experiment with large, deep networks (resnets)
        # experiment with:  
            # perturbations to inputs?
            # vulnerability to deletion of neurons?
            # interpretation of modules?
    # for SOC project, think about how to implement this in NGPU code!
        # it should be easy after doing it in this policy, as long ...
        # ... as you understand the NGPU code!
# List of future projects
    # Physics:
        # continuation of SOC project
        # investigation of Fisher information in the context of phase transitions
            # exploration below
        # refinement of 'Exponential Expressivity Through Transient Chaos"
            # to learning as opposed to gaussian initialization
            # connections to EWC?
            # connections to below point (learning as non-equilibrium stat mech?
        # spontaneous symmetry breaking -- Goldstone modes?
        # learning as non-equilibrium statistical mechanics
            # connections to Todd Gingrich Markov process entorpy bounds?
            # connections to Rao-Cramer "thermodynamic metric"
        # renormalization for initialization of larger network
        # renormalization for continual or (especially!) transfer learning.
    # Attention:
        # Attentive RNN image recognition less susceptible to adversarial prtbs?
        # Can working memory shape modular structure of network?
        # RRNN (Recursive Recurrent NNs):
            # like capsule network but more flexible
            # reusable capsules; dynamically determined depth
            # however, I like the idea of activation as feature existence probablty
            # Q: could these be good for transfer learning? Maybe.
        # Tensor valued working memories (generalizing NTM)
# List of job application plans
    # OpenAI fellowship
    # AI2 software engineer
    # AI for Brain Science -- look up positions
    # Google DeepMind -- research engineer
